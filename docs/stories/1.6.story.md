# Story 1.6: Infrastructure Optimization

## Status

**Draft**

---

## Story

**As a** platform operator,
**I want** right-sized Kubernetes resources and optimized connection pooling,
**so that** infrastructure costs are minimized while maintaining performance SLAs.

---

## Acceptance Criteria

1. Pod resource requests/limits right-sized based on metrics
2. Connection pooling configured for MongoDB and Redis
3. HPA thresholds tuned based on actual usage patterns
4. Resource utilization improved (target: 60-80% efficiency)
5. Cost reduction documented

---

## Tasks / Subtasks

- [ ] **Task 1: Resource Usage Analysis** (AC: 1, 4)
  - [ ] 1.1 Collect pod metrics via `kubectl top pods` over 1-week period
  - [ ] 1.2 Analyze CPU/memory usage patterns for backend, frontend, redis
  - [ ] 1.3 Identify over-provisioned pods (< 30% utilization)
  - [ ] 1.4 Identify under-provisioned pods (> 90% utilization)
  - [ ] 1.5 Document current vs recommended resource allocations

- [ ] **Task 2: Pod Resource Right-Sizing** (AC: 1, 4)
  - [ ] 2.1 Update backend deployment resource requests/limits
  - [ ] 2.2 Update frontend deployment resource requests/limits
  - [ ] 2.3 Update redis deployment resource requests/limits
  - [ ] 2.4 Validate pods schedule correctly after changes
  - [ ] 2.5 Monitor for OOMKills or CPU throttling

- [ ] **Task 3: Connection Pooling Configuration** (AC: 2)
  - [ ] 3.1 Review MongoDB connection pooling settings in `motor` client
  - [ ] 3.2 Review Redis connection pooling in `redis-py` client
  - [ ] 3.3 Configure optimal pool sizes based on pod count and workload
  - [ ] 3.4 Add connection pool metrics to health endpoint
  - [ ] 3.5 Test connection handling under load

- [ ] **Task 4: HPA Tuning** (AC: 3)
  - [ ] 4.1 Review current HPA configuration (if exists)
  - [ ] 4.2 Analyze traffic patterns to determine scaling triggers
  - [ ] 4.3 Configure CPU/memory-based scaling thresholds
  - [ ] 4.4 Set appropriate min/max replica counts
  - [ ] 4.5 Test scaling behavior with load simulation

- [ ] **Task 5: Cost Documentation** (AC: 5)
  - [ ] 5.1 Document current infrastructure costs (compute, storage, networking)
  - [ ] 5.2 Calculate cost savings from resource optimization
  - [ ] 5.3 Create cost optimization recommendations document
  - [ ] 5.4 Update `docs/deployment/cost-optimization.md`

---

## Dev Notes

### Relevant Source Tree

```
.pipeline/
├── k8s/
│   ├── base/
│   │   ├── backend-deployment.yaml     # Backend pod config
│   │   ├── frontend-deployment.yaml    # Frontend pod config
│   │   └── redis-deployment.yaml       # Redis pod config
│   └── overlays/
│       ├── prod/
│       │   └── backend-prod-patch.yaml # Production overrides
│       └── test/
│           └── backend-test-patch.yaml # Test overrides

backend/
├── src/
│   ├── database/
│   │   ├── mongodb.py                  # MongoDB connection handling
│   │   └── redis.py                    # Redis connection handling
│   └── core/
│       └── config.py                   # Connection settings
```

### Technical Context

**Current Deployment**:
- Platform: Alibaba Cloud ACK (Shanghai)
- Namespace: `klinematrix-prod`
- Registry: Azure ACR (`financialagent-gxftdbbre4gtegea.azurecr.io`)

**Current Resource Allocations** (to be verified):
```yaml
# Example backend pod resources
resources:
  requests:
    memory: "512Mi"
    cpu: "250m"
  limits:
    memory: "1Gi"
    cpu: "500m"
```

**Connection Pooling Context**:
- MongoDB: `motor` async client (default pool_max_size=100)
- Redis: `redis-py` async client (connection_pool)

**Scaling Considerations**:
- Backend pods handle API + LLM requests (bursty)
- Frontend pods serve static assets (low resource)
- Redis is single instance (consider sentinel for HA if needed)

### Previous Story Insights

**Story 1.1 Findings**:
- Infrastructure resource utilization baseline established
- Potential over-provisioning identified in some pods

**CLAUDE.md Reference**:
- Node pool limits are immutable (max-pods cannot be changed after creation)
- High-memory workloads (2Gi request) → userpoolv2 (Standard_E2_v3, 16GB)
- Lightweight workloads (< 256Mi) → userpool (Standard_D2ls_v5, 4GB)

---

## Testing

### Testing Standards [Source: docs/development/testing-strategy.md]

**Test Location**: Manual verification

**Infrastructure Testing**:
- Use `kubectl top pods` for real-time metrics
- Monitor via Kubernetes Dashboard or Lens
- Health endpoint: `GET /api/health`

**For This Story**:
- Load testing with k6 or similar to verify scaling
- Connection pool exhaustion testing
- OOMKill and throttling monitoring

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-12-23 | 1.0 | Story created from epic definition | Bob (SM) |

---

## Dev Agent Record

### Agent Model Used

*(To be filled by Dev Agent)*

### Debug Log References

*(To be filled by Dev Agent)*

### Completion Notes List

*(To be filled by Dev Agent)*

### File List

*(To be filled by Dev Agent)*

---

## QA Results

*(To be filled by QA Agent)*
