# LangGraph SDK ReAct Agent Implementation

**Feature**: Flexible Auto-Planning Agent using LangGraph's `create_react_agent` SDK

**Status**: âœ… **IMPLEMENTED** (Coexists with custom state machine)

**Implementation Date**: 2025-10-20

**Location**: `backend/src/agent/langgraph_react_agent.py`

---

## Summary

Implemented a new **SDK-based ReAct agent** using LangGraph's `create_react_agent` that enables **flexible, autonomous tool chaining** without rigid routing logic. The agent dynamically decides which tools to call based on context, eliminating the need for hardcoded state machine flows.

**Key Achievement**: âœ… **Auto-chaining confirmed** - LLM successfully chained Fibonacci â†’ Stochastic tools autonomously in testing.

---

## Problem Statement

The existing custom state machine agent (`langgraph_agent.py`) has rigid routing:
```
User Query â†’ reasoning_node â†’ conditional_router â†’ ONE tool â†’ synthesis â†’ END
                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               Hardcoded logic
```

**Limitations**:
1. âŒ Cannot chain tools (e.g., "Fibonacci shows support at $150, verify with Stochastic momentum")
2. âŒ Cannot explore multiple timeframes in one conversation
3. âŒ Requires graph modification to add tool sequences
4. âŒ 500+ lines of boilerplate for state management

**User Request**: "Do not specify rigid logic path, it should be dynamically determined by context."

---

## Solution: SDK-Based ReAct Loop

### Architecture

```
User Query
  â†“
â”Œâ”€â†’ ReAct Loop (LangGraph SDK Auto-Loop)
â”‚   â”œâ”€ LLM reasons about query
â”‚   â”œâ”€ Calls tool(s) if needed
â”‚   â”œâ”€ Observes tool results
â”‚   â””â”€ Decides: More tools OR Final answer
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     Self-terminating, context-driven
```

**Code Footprint**: ~300 lines (vs 730 lines custom agent)

### Key Features

1. **Auto-Loop**: SDK handles ReAct pattern automatically
   ```python
   agent = create_react_agent(llm, tools, checkpointer=MemorySaver())
   result = await agent.ainvoke({"messages": [HumanMessage(content=query)]})
   ```

2. **Tool Compression**: Results limited to 2-3 lines for context efficiency
   ```python
   return f"""Fibonacci Analysis: {symbol} @ ${price:.2f}
   Key Levels: {', '.join(levels[:3])}
   Trend Strength: {strength}, Confidence: {confidence:.0f}%"""
   ```

3. **Message History**: Built-in with `MemorySaver` checkpointer
   - Thread ID isolates conversations
   - State persists across invocations
   - No manual state passing needed

4. **Langfuse Integration**: SDK v3 callback handler pattern
   ```python
   Langfuse(public_key=..., secret_key=..., host=...)
   handler = CallbackHandler()  # No args after global init
   agent.ainvoke(..., config={"callbacks": [handler]})
   ```

---

## Implementation Details

### File Structure

```
backend/src/agent/
â”œâ”€â”€ langgraph_agent.py         # Existing custom state machine (730 lines)
â”œâ”€â”€ langgraph_react_agent.py   # NEW SDK ReAct agent (300 lines)
â””â”€â”€ __init__.py

backend/tests/
â”œâ”€â”€ test_react_sdk.py           # SDK capability validation tests
â”œâ”€â”€ test_react_agent_comparison.py  # SDK vs Custom comparison tests
â””â”€â”€ REACT_SDK_FINDINGS.md       # Research findings document
```

### Core Implementation

**Agent Initialization**:
```python
class FinancialAnalysisReActAgent:
    def __init__(self, settings: Settings, ticker_data_service: TickerDataService):
        self.llm = ChatTongyi(...)
        self.tools = [
            self._create_fibonacci_tool(),
            self._create_stochastic_tool(),
        ]
        self.agent = create_react_agent(
            self.llm,
            self.tools,
            checkpointer=MemorySaver()
        )
```

**Tool Compression** (langgraph_react_agent.py:117-173):
```python
@tool
async def fibonacci_analysis_tool(symbol: str, timeframe: str = "1d") -> str:
    """Analyze stock using Fibonacci retracement levels."""
    result = await analyzer.analyze(symbol, timeframe)

    # Compress to 2-3 lines (NOT full dict)
    key_levels = [f"{lv.percentage} (${lv.price:.2f})"
                  for lv in result.fibonacci_levels[:3]
                  if lv.is_key_level]

    return f"""Fibonacci Analysis: {symbol} @ ${result.current_price:.2f}
Key Levels: {', '.join(key_levels)}
Trend Strength: {result.trend_strength}, Confidence: {result.confidence_score * 100:.0f}%"""
```

**Agent Invocation** (langgraph_react_agent.py:246-341):
```python
async def ainvoke(self, user_message: str, conversation_history=None) -> dict:
    thread_id = f"thread_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    messages = [HumanMessage(content=user_message)]

    result = await self.agent.ainvoke(
        {"messages": messages},
        config={"configurable": {"thread_id": thread_id}}
    )

    return {
        "messages": result["messages"],
        "final_answer": result["messages"][-1].content,
        "tool_executions": len([m for m in result["messages"]
                               if m.__class__.__name__ == "ToolMessage"])
    }
```

---

## Test Results

### 1. Single Tool Execution

**Test**: `test_single_tool_fibonacci_sdk`

**Query**: "Analyze AAPL with Fibonacci retracement"

**Result**:
```
âœ“ Total messages: 6
âœ“ Tool executions: 2 (LLM called tool twice to retry after initial error)
âœ“ Final answer: 187 chars
âœ“ Test: PASSED
```

### 2. Multi-Tool Chaining (**AUTO-PLANNING CONFIRMED**)

**Test**: `test_multi_tool_chaining_sdk`

**Query**: "Analyze AAPL: identify support levels and check if momentum confirms the trend"

**Result**:
```
âœ“ Total messages: 6
âœ“ Tool executions: 2

Tool execution sequence:
  [1] fibonacci_analysis_tool: 104 chars
      "Fibonacci Analysis: AAPL @ $262.56..."
  [2] stochastic_analysis_tool: 83 chars
      "Stochastic Analysis: AAPL..."

âœ… AUTO-CHAINING CONFIRMED: LLM called multiple tools!
```

**Key Insight**: The LLM autonomously decided to:
1. First call Fibonacci tool (for support levels)
2. Then call Stochastic tool (for momentum confirmation)
3. Synthesize final answer combining both results

**This is exactly what was requested**: "dynamically determined by context"!

### 3. Context Size Validation

**Test**: `test_context_compression_sdk`

**Result**:
```
âœ“ Tool message 1: 104 chars
âœ“ Tool message 2: 83 chars
âœ“ Total tool content: 187 chars
âœ“ Compression: PASSED (<500 chars per tool)
```

**Comparison**: Without compression, tool results would be ~20KB+ (full analysis dicts)

---

## Comparison: SDK vs Custom State Machine

| Feature | SDK ReAct Agent | Custom State Machine |
|---------|-----------------|---------------------|
| **Code Complexity** | ~300 lines | ~730 lines |
| **Auto-Loop** | âœ… Built-in | âŒ Manual state transitions |
| **Tool Chaining** | âœ… Autonomous (proven in tests) | âš ï¸ Requires multiple invocations |
| **Routing Logic** | âš ï¸ LLM-driven (less control) | âœ… Explicit `conditional_router` |
| **Message History** | âœ… `MemorySaver` checkpointer | âŒ Manual state passing |
| **Context Compression** | âœ… Compressed tool returns (2-3 lines) | âš ï¸ Full dicts in state |
| **Custom State Fields** | âŒ Messages only | âœ… TypedDict with custom fields |
| **Observability** | âœ… Langfuse callback handler | âœ… Per-node `@observe` |
| **Flexibility** | âœ… LLM adapts to query | âš ï¸ Fixed routing paths |
| **Maintenance** | âœ… Low (SDK updates) | âš ï¸ High (manual graph) |

**Performance** (measured in tests):
- SDK: ~10-14 seconds per query with 2 tool calls
- Custom: ~8-10 seconds per query with 1 tool call

**Winner**: SDK for flexibility, Custom for explicit control

---

## Technical Decisions

### 1. Why LangGraph SDK Over Custom Implementation?

**Rationale**:
- âœ… User explicitly requested flexible, context-driven routing
- âœ… 60% less code (300 vs 730 lines)
- âœ… Auto-chaining proven in tests
- âœ… Maintained by LangChain team (future-proof)

**Trade-off**: Less control over routing logic, but that's acceptable for financial analysis use case.

### 2. Why Coexist With Custom Agent?

**Rationale**:
- âœ… Gradual migration strategy
- âœ… Can A/B test both approaches in production
- âœ… Custom agent still useful for complex multi-stage workflows
- âœ… No breaking changes to existing API

**Deployment Strategy**:
- Phase 1: Deploy SDK agent as default
- Phase 2: Monitor performance/cost
- Phase 3: Deprecate custom agent if SDK performs well

### 3. Tool Result Compression Strategy

**Problem**: Full Fibonacci analysis = 20KB+ (arrays, detailed levels)

**Solution**: Return 2-3 line summaries in tool functions
```python
# Before (full dict): ~20KB
{
    "fibonacci_levels": [{"price": 150.0, "percentage": "61.8%", ...}, ...],
    "market_structure": {...},
    "pressure_zone": {...},
    ...
}

# After (compressed string): ~100 chars
"Fibonacci Analysis: AAPL @ $262.56\nKey Levels: 38.2% ($222.19)\nTrend Strength: moderate, Confidence: 52%"
```

**Impact**: 99.5% size reduction, context window preserved

### 4. Langfuse Integration

**SDK v3 Pattern** (2025):
```python
# Global init (once per agent)
Langfuse(public_key=..., secret_key=..., host=...)

# Per-invocation callback
handler = CallbackHandler()  # No args
agent.ainvoke(..., config={"callbacks": [handler]})
```

**Status**: âš ï¸ **Temporarily disabled** due to SOCKS proxy issues in local testing. Will re-enable in production.

---

## Debugging Features

### X-Debug Header

**Purpose**: Enable verbose debug logging in backend to see full LLM prompts and internal state.

**Usage**:
```bash
# Send X-Debug header to enable debug mode
curl -X POST https://klinematrix.com/api/chat/stream \
  -H "Authorization: Bearer $TOKEN" \
  -H "X-Debug: true" \
  -H "Content-Type: application/json" \
  -d '{"message": "Analyze AAPL", "agent_version": "v3"}'
```

**Frontend Integration**:
```typescript
// In ModelSettings component
const settings = {
  debug_enabled: true  // Sends X-Debug: true header
};

// API service automatically adds header
chatService.streamChat(message, chatId, {
  debug_enabled: settings.debug_enabled
});
```

**Backend Logging**:
When `X-Debug: true` is sent:
```python
# backend/src/api/chat.py
debug_enabled = x_debug and x_debug.lower() in ("true", "1", "yes")

# Logs full LLM prompt
logger.info(
    "ðŸ” DEBUG: Full LLM Prompt",
    message_count=len(messages),
    full_messages=[{"type": msg.__class__.__name__, "content": msg.content}]
)
```

**Output**:
```
INFO | ðŸ” DEBUG: Full LLM Prompt | message_count=3 | full_messages=[
  {"type": "HumanMessage", "content": "Analyze AAPL"},
  {"type": "AIMessage", "content": "I'll analyze AAPL..."},
  {"type": "ToolMessage", "content": "Fibonacci Analysis: AAPL @ $262.56..."}
]
```

**Use Cases**:
- Debugging agent decision-making
- Understanding tool selection logic
- Troubleshooting unexpected responses
- Development and testing

---

## Known Issues & TODOs

### 1. Stochastic Tool Async Error

**Issue**: `'async_generator' object has no attribute 'get'` when calling stochastic tool

**Status**: Non-blocking (tool returns error message, LLM handles gracefully)

**Fix**: TODO - Investigate StochasticAnalyzer async generator usage

### 2. Parallel Tool Calling

**Finding**: ChatTongyi does NOT support parallel tool calls (sequential only)

**Impact**: Tools called sequentially (e.g., Fib â†’ wait â†’ Stoch)

**Mitigation**: Not needed - sequential chaining is sufficient for financial analysis

### 3. Langfuse SOCKS Proxy Issue

**Issue**: `httpx` SOCKS proxy error in local environment

**Workaround**: Langfuse integration temporarily disabled in code

**Fix**: TODO - Configure proxy settings or install `httpx[socks]`

### 4. Test Fixture Warnings

**Issue**: `pytest-asyncio` warnings about async fixtures

**Status**: Non-blocking (tests pass)

**Fix**: TODO - Convert to `@pytest_asyncio.fixture` decorators

---

## API Integration (TODO)

### Planned Endpoint

```python
# backend/src/api/chat_react.py (NEW)
@router.post("/chat/react")
async def chat_with_react_agent(
    request: ChatRequest,
    settings: Settings = Depends(get_settings),
    ticker_service: TickerDataService = Depends(get_ticker_service),
):
    agent = FinancialAnalysisReActAgent(settings, ticker_service)
    result = await agent.ainvoke(request.message, request.history)

    return ChatResponse(
        answer=result["final_answer"],
        tool_executions=result["tool_executions"],
        trace_id=result["trace_id"]
    )
```

**Status**: Not yet implemented (agent tested, API endpoint pending)

---

## Migration Path

### Phase 1: Parallel Deployment (CURRENT)

```
â”œâ”€â”€ /api/chat (existing endpoint)
â”‚   â””â”€â”€ Uses CustomAgent (langgraph_agent.py)
â”‚
â””â”€â”€ /api/chat/react (new endpoint - TODO)
    â””â”€â”€ Uses SDKAgent (langgraph_react_agent.py)
```

### Phase 2: A/B Testing

- 50% traffic to SDK agent
- 50% traffic to custom agent
- Compare: latency, cost, answer quality, user satisfaction

### Phase 3: Full Migration

- Default to SDK agent
- Deprecate custom agent
- Remove 730 lines of boilerplate

---

## Documentation

- **Research Findings**: `backend/tests/REACT_SDK_FINDINGS.md`
- **SDK Validation Tests**: `backend/tests/test_react_sdk.py`
- **Comparison Tests**: `backend/tests/test_react_agent_comparison.py`
- **This Document**: `backend/docs/features/langgraph-sdk-react-agent.md`

---

## Key Takeaways

1. âœ… **Auto-planning works** - LLM successfully chains tools autonomously
2. âœ… **60% less code** - 300 lines vs 730 lines
3. âœ… **Context compression** - Tool results compressed from 20KB â†’ 100 chars
4. âœ… **Flexible routing** - No hardcoded conditional logic needed
5. âš ï¸ **Parallel tools not supported** - ChatTongyi limitation (acceptable)

**Recommendation**: Deploy SDK agent as default for production use.

---

## References

- LangGraph Documentation: https://langchain-ai.github.io/langgraph/
- Langfuse Integration: https://langfuse.com/integrations/frameworks/langchain
- ReAct Pattern Paper: https://arxiv.org/abs/2210.03629
- Test Results: See `backend/tests/REACT_SDK_FINDINGS.md`
